{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\HP\\\\Desktop\\\\datasets'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "os.chdir('C://Users//HP//Desktop//datasets')\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "d=pd.read_excel(io='C://Users//HP//Desktop//datasets//bppfinal2.xls')\n",
    "d1=pd.read_excel(io='C://Users//HP//Desktop//datasets//schfinal2.xlsx')\n",
    "d2=pd.read_excel(io='C://Users//HP//Desktop//datasets//ocdfinal2.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = d[['Statement', 'Disorder']]\n",
    "train = [tuple(x) for x in subset.values]\n",
    "\n",
    "subset = d1[['Statement', 'Disorder']]\n",
    "train1 = [tuple(x) for x in subset.values]\n",
    "\n",
    "subset = d2[['Statement', 'Disorder']]\n",
    "train2 = [tuple(x) for x in subset.values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer,WordNetLemmatizer\n",
    "lem=WordNetLemmatizer()\n",
    "ss=SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "words = set(word.lower() for passage in train for word in word_tokenize(passage[0]))\n",
    "all_words=set(ss.stem(lem.lemmatize(w)) for w in words)\n",
    "words1 = set(word.lower() for passage in train1 for word in word_tokenize(passage[0]))\n",
    "all_words1=set(ss.stem(lem.lemmatize(w)) for w in words1)\n",
    "words2 = set(word.lower() for passage in train2 for word in word_tokenize(passage[0]))\n",
    "all_words2=set(ss.stem(lem.lemmatize(w)) for w in words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1]) for x in train]\n",
    "t1 = [({word: (word in word_tokenize(x[0])) for word in all_words1}, x[1]) for x in train1]\n",
    "t2 = [({word: (word in word_tokenize(x[0])) for word in all_words2}, x[1]) for x in train2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something:\n",
      "You said\n",
      "today is a very good day and I am very proud of it and I am feeling extremely happy\n",
      "Speech recognition completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['today is a very good day and i am very proud of it and i am feeling extremely happy']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "r=sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    r.adjust_for_ambient_noise(source)\n",
    "    print(\"Say something:\")\n",
    "    audio=r.listen(source)\n",
    "test_sentence=\"\" \n",
    "try:\n",
    "    print(\"You said\\n\"+r.recognize_google(audio))  \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "test_sentence=test_sentence+r.recognize_google(audio)\n",
    "print(\"Speech recognition completed\")\n",
    "sentences=sent_tokenize(test_sentence.lower())\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                       , = True           bipola : normal =      7.1 : 1.0\n",
      "                    feel = True           bipola : normal =      6.9 : 1.0\n",
      "                      of = True           bipola : normal =      5.1 : 1.0\n",
      "                      on = True           bipola : normal =      5.1 : 1.0\n",
      "                    have = True           bipola : normal =      4.8 : 1.0\n",
      "                     and = True           bipola : normal =      4.8 : 1.0\n",
      "                     n't = True           bipola : normal =      4.4 : 1.0\n",
      "                    that = True           bipola : normal =      4.2 : 1.0\n",
      "                      am = True           normal : bipola =      4.2 : 1.0\n",
      "                   about = True           bipola : normal =      3.7 : 1.0\n",
      "Most Informative Features\n",
      "                    that = True           Schizo : normal =     16.1 : 1.0\n",
      "                       , = True           Schizo : normal =      7.8 : 1.0\n",
      "                    what = True           Schizo : normal =      7.3 : 1.0\n",
      "                    feel = True           Schizo : normal =      5.7 : 1.0\n",
      "                      me = True           Schizo : normal =      5.0 : 1.0\n",
      "                       . = False          normal : Schizo =      4.8 : 1.0\n",
      "                    have = True           Schizo : normal =      4.7 : 1.0\n",
      "                   about = True           Schizo : normal =      4.4 : 1.0\n",
      "                   trust = True           Schizo : normal =      4.4 : 1.0\n",
      "                     get = True           Schizo : normal =      4.4 : 1.0\n",
      "Most Informative Features\n",
      "                   about = True              OCD : normal =     14.5 : 1.0\n",
      "                       , = True              OCD : normal =     14.5 : 1.0\n",
      "                 perform = True              OCD : normal =     13.1 : 1.0\n",
      "                      on = True              OCD : normal =      7.6 : 1.0\n",
      "                    that = True              OCD : normal =      7.0 : 1.0\n",
      "                       . = True           normal : OCD    =      5.6 : 1.0\n",
      "                      of = True              OCD : normal =      5.0 : 1.0\n",
      "                      an = True              OCD : normal =      4.8 : 1.0\n",
      "                     and = True              OCD : normal =      4.3 : 1.0\n",
      "                       a = True              OCD : normal =      3.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "classifier1 = nltk.NaiveBayesClassifier.train(t1)\n",
    "classifier2 = nltk.NaiveBayesClassifier.train(t2)\n",
    "classifier.show_most_informative_features()\n",
    "classifier1.show_most_informative_features()\n",
    "classifier2.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " \"'today\",\n",
       " 'is',\n",
       " 'a',\n",
       " 'very',\n",
       " 'good',\n",
       " 'day',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'very',\n",
       " 'proud',\n",
       " 'of',\n",
       " 'it',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'feeling',\n",
       " 'extremely',\n",
       " 'happy',\n",
       " \"'\",\n",
       " ']']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=word_tokenize(str(sentences))\n",
    "sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', \"'today\", 'is', 'a', 'very', 'good', 'day', 'and', 'i', 'am', 'very', 'proud', 'of', 'it', 'and', 'i', 'am', 'feeling', 'extremely', 'happy', \"'\", ']']\n",
      "[\"'today\", 'good', 'day', 'proud', 'feeling', 'extremely', 'happy']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words =set(stopwords.words('english'))\n",
    "filtered_sentence=[w for w in sentences if not w in stop_words]\n",
    "\n",
    "punctuations='''i!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "filtered_sentence=[]\n",
    "for w in sentences:\n",
    "    if w not in stop_words and w not in punctuations:\n",
    "        filtered_sentence.append(w)\n",
    "        \n",
    "print(sentences)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'today\": True,\n",
       " 'day': False,\n",
       " 'extremely': False,\n",
       " 'feeling': False,\n",
       " 'good': False,\n",
       " 'happy': False,\n",
       " 'proud': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent_features = {word.lower():(word in word_tokenize(ss.stem(lem.lemmatize(str(filtered_sentence))))) for word in filtered_sentence}\n",
    "test_sent_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bp:normal\n",
      "sch:normal\n",
      "ocd:normal\n"
     ]
    }
   ],
   "source": [
    "print(\"bp:\"+classifier.classify(test_sent_features))\n",
    "print(\"sch:\"+classifier1.classify(test_sent_features))\n",
    "print(\"ocd:\"+classifier2.classify(test_sent_features))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
